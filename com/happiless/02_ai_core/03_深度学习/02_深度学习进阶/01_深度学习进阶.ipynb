{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 激活函数\n",
    "##### $$ Sigmoid = \\frac{1}{1 + e^{-x}} $$\n",
    "##### $$ tanh = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "##### $$ Relu = max(0, x) $$\n",
    "##### $$ LeakyRelu = \\alpha x (x >0, \\alpha = 1)$$\n",
    "\n",
    "### 初始化\n",
    "##### Uniform Distribution： Xavier initialization\n",
    "##### Normal Distribution with mean 0 and standard deviation\n",
    "$$ \\sigma = \\sqrt{\\frac{2}{n_inputs + n_outputs}} $$\n",
    "##### Uniform Distribution between-r and +r , with\n",
    "$$ r = \\sqrt{\\frac{6}{n_inputs + n_outputs}}$$\n",
    "\n",
    "### 梯度消失与梯度爆炸\n",
    "\n",
    "##### 因为在反向传播的链式求导中，因为连乘的原因会导致梯度要么变得特别小，要么变得特别大\n",
    "- 1. 梯度剪切， 正则\n",
    "- 2. 采用其他激活函数，Relu， LeakyRelu\n",
    "- 3. Batch Normalization\n",
    "- 4. 残差结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Batch Normalization\n",
    "![image.png](./Batch_Normlization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Gradient clipping\n",
    "-   1. 直接将过大的梯度值变小\n",
    "\n",
    "### Gradient chekcing 梯度检查\n",
    "\n",
    "### learning rate  and stop early\n",
    "-   1. 我们可以设置成learning-rate和loss是一个相关的函数\n",
    "-   2. Loss越小，learning-rate越小(learning rate decay)\n",
    "-   3. 当我们发现loss连续k次不下降的时候，我们可以提前停止训练过程，这个方法叫做“early stop”\n",
    "\n",
    "### Optimizer 不同的优化方法\n",
    "-   1. SGD\n",
    "-   2. RMSprop\n",
    "-   3. Adam\n",
    "-   4. Adadelta\n",
    "-   5. Adagrad\n",
    "-   6. Adamax\n",
    "-   7. Nadam\n",
    "-   8. Ftrl\n",
    "\n",
    "### Gradient with momentum (梯度动量法): 动量法基于梯度\n",
    "-   1. 梯度更新的规则\n",
    "$$ w_{t+1} = w_t - \\eta \\delta w_t$$\n",
    "-   2. 动量基于梯度更新规则\n",
    "$$ v_t = \\gamma * v_{t-1} + \\eta \\delta w_t$$\n",
    "$$ w_{t+1} = w_t - v_t$$\n",
    "-   3. RMS-PROP\n",
    "$$ S\\frac{\\delta_{loss}}{\\delta_w} = \\beta S \\frac{\\delta_{loss}}{\\delta_w} + (1 - \\beta) ||\\frac{\\delta_{loss}}{\\delta_w}||^2$$\n",
    "\n",
    "-   4. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 如何选择优化算法\n",
    "-   1. 如果数据是稀疏的，就用自适用方法，即 Adagrad, Adadelta, RMSprop, Adam\n",
    "-   2. RMSprop, Adadelta, Adam 在很多情况下的效果是相似的。\n",
    "-   3. Adam 就是在 RMSprop 的基础上加了 bias-correction 和 momentum，\n",
    "-   4. 随着梯度变的稀疏，Adam 比 RMSprop 效果会好。\n",
    "-   5. 整体来讲，Adam 是最好的选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adam():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSprop():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adadelta():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Adagrad():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.SGD\n",
    "optim.Adam\n",
    "optim.Adadelta\n",
    "optim.Adagrad\n",
    "optim.RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
