{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 混淆矩阵\n",
    "    准确率(Accuracy): (TP+TN) / (TP + FP  + FN + TN)\n",
    "    精确率(Precision): TP / (TP + FP)\n",
    "    召回率(Recall): TP / (TP + FN)\n",
    "    F1-score: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    F2-score: (1 + beta ** 2) * (Precision * Recall) / beta ** 2 * Precision + Recall\n",
    "        F1是一个用来衡量Precision和Recall的一个指标，往往当F1很高时，Precision和Recall也很高。\n",
    "    AUC\n",
    "        AUC是一个衡量模型的指标，当AUC趋近于1，说明该模型的分类效果越好；反之，若AUC趋近于0，说明该模型分类效果越差。\n",
    "        AUC一般在数据不平衡的情况下使用。（邮件、疾病、推荐）\n",
    "\n",
    "## 作业\n",
    "##### 1. 实现softmax，cross-entropy，并且说明他们的使用场景和作用意义；\n",
    "    softmax是将逻辑回归的输出转换成一组概率， 且这组概率为0-1之间，和为1的概率分布，softmax是一个多分类模型，\n",
    "    而交叉熵cross_entropy是softmax的损失函数，因为softmax输出的是概率分布，衡量概率分布的相似程度的常用方法是KL散度，\n",
    "    然后，KL散度和交叉熵几乎是一回事。理论上softmax也可以搭配其他的概率分布，例如\n",
    "    loss = sum(y_i * p + (1-yi) * (1 - p))， 但是cross_entropy函数使用了log变换，会在损失远离0时更快的收敛，"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.24232893, 0.72799634, 0.02967474])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    x -= np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "softmax([1.3, 2.4, -0.8])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "2.279028485862769"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(label, s):\n",
    "    return - sum([label[i] * np.log(s[i]) for i in range(len(label))])\n",
    "\n",
    "L = [0.10, 0.40, 0.50]\n",
    "S = [0.80, 0.15, 0.05]\n",
    "cross_entropy(L, S)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2. 总结过拟合、欠拟合的作用和意义；\n",
    "    过拟合是在训练集上拟合很好，在测试集上拟合不好\n",
    "        原因：\n",
    "            1. 训练样本抽取不平衡，导致训练集数据过少\n",
    "            2. 训练集和测试集分布不一致\n",
    "            3. 特征参数过多，模型过于复杂\n",
    "            4. 权值学习迭代次数过多，拟合了噪声数据\n",
    "        解决方案：\n",
    "            1. 数据增强，增加训练数据\n",
    "            2. 随机抽样，使训练集和测试集数据比较均衡\n",
    "            3. 适当减小模型的复杂度，引入正则项惩罚机制\n",
    "            4. early stopping\n",
    "    欠拟合是在训练集上拟合不好，在测试集上拟合很好\n",
    "        原因：\n",
    "            1. 特征量过少\n",
    "            2. 特征参数太少， 模型过于简单\n",
    "        解决方案：\n",
    "            1. 进行升维，增加有效影响特征或者多项式特征\n",
    "            2. 使用复杂度更高的非线性模型，例如SVM, 决策树, 深度学习等"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 3. 总结AUC的作用和意义；\n",
    "    AUC 是一个衡量模型的指标, 当AUC趋近于1，说明该模型的分类效果越好；反之若AUC趋近于0，说明该模型分类效果越差\n",
    "    AUC 一般在数据不平衡的情况下使用，例如邮件分类，疾病预测，推荐系统等\n",
    "\n",
    "##### 4. 城市里边有一共200万个居民，其中有100人是犯罪分子，然后警察随机找了一批人，这批人一共200人：\n",
    "    现在定义犯罪分析为positive samples, 那么\n",
    "    case1: 警察定义的这200人中，一共有犯罪分子实际有89人，警察判定是犯罪分子的人一共有80人，这80人中，真正是犯罪分子的有79名；\n",
    "预测/真实 | 坏人 | 好人\n",
    "---|---|---\n",
    "坏人 | TP: 79 | FP: 1\n",
    "好人 | FN: 10 | TN: 110\n",
    "\n",
    "        请问，警察判断的accuracy是多少，recall是多少，precision是多少？\n",
    "            accuracy = (TP + TN) / (TP + FP + FN + TN) = (89 + 110) / 200 = 94.5%\n",
    "            recall = TP / (TP + FN) = 79 / 89 = 89%\n",
    "            precision = TP / (TP + FP) = 79 / 80 = 98.75%\n",
    "    case2: 警察定义的这200人中，一共有犯罪分子实际有100人，警察判定是犯罪分子的人一共有200人，这200人中，真正是犯罪分子的有100名；\n",
    "预测/真实 | 坏人 | 好人\n",
    "---|---|---\n",
    "坏人 | TP: 100 | FP: 100\n",
    "好人 | FN: 0 | TN: 0\n",
    "\n",
    "        请问，警察判断的accuracy是多少，recall是多少，precision是多少？\n",
    "            accuracy = (TP + TN) / (TP + FP + FN + TN) = 100 / 200 = 50%\n",
    "            recall = TP / (TP + FN) = 100 / 100 = 100%\n",
    "            precision = TP / (TP + FP) = 100 / 200 = 50%\n",
    "\n",
    "##### 5. one-hot的意义以及代码实现方法；\n",
    "    one-hot编码的作用：\n",
    "        1. 使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。\n",
    "        2. 对离散型特征进行one-hot编码是为了让距离的计算显得更加合理。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame([{'name': '张三', 'sex': 'Male', 'age': 15},\n",
    "                     {'name': '李四', 'sex': 'Unknow', 'age': 35},\n",
    "                     {'name': '王五', 'sex': 'Female', 'age': 18},\n",
    "                     {'name': '王五', 'sex': 'Female', 'age': 18}\n",
    "                     ])\n",
    "def one_hot_encoder_by_sklearn(data):\n",
    "    encoder = preprocessing.OneHotEncoder()\n",
    "    encoder.fit(data['sex'].values.reshape(-1,1))\n",
    "    ans = encoder.transform([['Male'], ['Unknow'], ['Female']]).toarray()\n",
    "    print(ans)\n",
    "one_hot_encoder_by_sklearn(data)\n",
    "\n",
    "def one_hot_encoder_by_pandas(data):\n",
    "    result = pd.get_dummies(data, columns=['sex'])\n",
    "    result.head()\n",
    "    return result\n",
    "one_hot_encoder_by_pandas(data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 0, 23]\n",
      "[1, 0, 0, 0, 0, 0, 0, 24]\n",
      "[0, 0, 0, 0, 1, 0, 0, 24]\n"
     ]
    }
   ],
   "source": [
    "# 自定义onehot编码\n",
    "cities = [\n",
    "    '北京',\n",
    "    '南京',\n",
    "    '大连',\n",
    "    '青岛',\n",
    "    '上海',\n",
    "    '深圳',\n",
    "    '杭州'\n",
    "]\n",
    "person1 = ['北京', 23]\n",
    "person2 = ['南京', 24]\n",
    "person3 = ['北京', 24]\n",
    "\n",
    "def user_define_one_hot_encoder(features):\n",
    "    from collections import defaultdict\n",
    "    one_hot_encoder = defaultdict(lambda : [0] * len(set(features)))\n",
    "    for i, f in enumerate(set(features)):\n",
    "        one_hot_encoder[f][i] = 1\n",
    "    return one_hot_encoder\n",
    "\n",
    "one_hot_encoder = user_define_one_hot_encoder(cities)\n",
    "\n",
    "def user_define_one_hot_decoder(features, encoder):\n",
    "    v = []\n",
    "    for f in features:\n",
    "        if f in encoder: v += encoder[f]\n",
    "        else:\n",
    "            v.append(f)\n",
    "    return v\n",
    "\n",
    "print(user_define_one_hot_decoder(person1, one_hot_encoder))\n",
    "print(user_define_one_hot_decoder(person2, one_hot_encoder))\n",
    "print(user_define_one_hot_decoder(person3, one_hot_encoder))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 6. normalization， standardization 的意义和代码实现方法。\n",
    "    normalization 和 standardization 是差不多的，都是将数据进行预处理，\n",
    "    normalization一般将数据限制在某个范围，比如一般都是[0, 1]， 从而消除数据量纲对建模的影响。\n",
    "    standardization 一般是指将数据正态化，使平均值为0方差为1， 因此normalization和standardization\n",
    "    是针对数据而言的，消除一些数值差异带来的特征重要性偏见，经过归一化的数据，能加快训练速度，促进算法的收敛\n",
    "    包括的方法主要有：\n",
    "        最大最小值归一化： MinMaxScaler\n",
    "        标准差归一化：StandardScaler"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "incomes = [[111212131], [32313111], [32431131], [34353311]]\n",
    "\n",
    "def scale(data, scaler):\n",
    "    print('\\nbefore processing')\n",
    "    print(incomes)\n",
    "\n",
    "    scaler.fit(incomes)\n",
    "    minmax_incomes = scaler.transform(incomes)\n",
    "    print('\\nafter scale')\n",
    "    print(minmax_incomes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "before processing\n",
      "[[111212131], [32313111], [32431131], [34353311]]\n",
      "\n",
      "after scale\n",
      "[[1.        ]\n",
      " [0.        ]\n",
      " [0.00149584]\n",
      " [0.02585837]]\n"
     ]
    }
   ],
   "source": [
    "# 最大最小值归一化\n",
    "minmax = MinMaxScaler()\n",
    "scale(incomes, minmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "before processing\n",
      "[[111212131], [32313111], [32431131], [34353311]]\n",
      "\n",
      "after scale\n",
      "[[ 1.73155534]\n",
      " [-0.59843008]\n",
      " [-0.59494481]\n",
      " [-0.53818046]]\n"
     ]
    }
   ],
   "source": [
    "# 标准差归一化\n",
    "standard = StandardScaler()\n",
    "scale(data, standard)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.        ],\n       [0.        ],\n       [0.00149584],\n       [0.02585837]])"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 另一种求解方式\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "minmax_scale(incomes)\n",
    "\n",
    "from sklearn.preprocessing import scale as standard_scale\n",
    "standard_scale(incomes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}